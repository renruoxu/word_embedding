{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning notes taken from \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\" by Omer Levy, Yoav Goldberg, and Ido Dagan. TACL 2015.\n",
    "- The purpose of the notes is to understand the paper and the author's implementation methods - we tried different parameter settings and different svd libraries.\n",
    "- the original codes are a mix of linux-commands and python scripts, we try to redo everything in python\n",
    "- all credits go to the authors of orignal paper: Omer Levy, Yoav Goldberg, and Ido Dagan\n",
    "- [paper link \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\"](https://levyomer.wordpress.com/2015/03/30/improving-distributional-similarity-with-lessons-learned-from-word-embeddings/)\n",
    "- [bitbucket code repository](https://bitbucket.org/omerlevy/hyperwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpus2pairs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import sqrt\n",
    "from random import Random\n",
    "\n",
    "rnd = Random(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_file = \"data/text8\"\n",
    "\n",
    "thr = 100 ## lowest freq for rare words\n",
    "\n",
    "subsample = 1e-5 ## high freq stop-words\n",
    "\n",
    "dyn = True ## dynamic window\n",
    "win = 3 ## window length\n",
    "\n",
    "pos = False ## pos matters in context\n",
    "\n",
    "d3l = True ## dirty play or not\n",
    "\n",
    "cds = 0.75 ## context distribution smoothing\n",
    "\n",
    "dim = 200 ## dimension for learned word vectors\n",
    "\n",
    "neg = 1. ## k for negative sampling\n",
    "\n",
    "normalize = True ## normalize the word vector matrix\n",
    "\n",
    "eig = 0.5 ## eigen value weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_vocab(corpus_file, thr):\n",
    "    vocab = Counter()\n",
    "    with open(corpus_file) as f:\n",
    "        for line in f:\n",
    "            vocab.update(Counter(line.strip().split()))\n",
    "    return dict([(token, count) for token, count in vocab.items() if count >= thr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 s, sys: 3.28 s, total: 15.7 s\n",
      "Wall time: 15.8 s\n",
      "15471435\n"
     ]
    }
   ],
   "source": [
    "%time vocab = read_vocab(corpus_file, thr)\n",
    "corpus_size = sum(vocab.values())\n",
    "print corpus_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsample *= corpus_size\n",
    "subsampler = dict([ (word, 1 - sqrt(subsample / count)) for word, count in vocab.items() if count > subsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs = []\n",
    "with open(corpus_file) as f:\n",
    "    for line in f:\n",
    "        tokens = [t if t in vocab else None for t in line.strip().split()] ## del-rare filter\n",
    "        if subsample != 0:\n",
    "            tokens = [t if t not in subsampler or rnd.random() > subsampler[t] else None \n",
    "                          for t in tokens] ##subsampling filter\n",
    "        if d3l:\n",
    "            tokens = [t for t in tokens if t is not None]\n",
    "        len_tokens = len(tokens)\n",
    "        \n",
    "        for i, tok in enumerate(tokens):\n",
    "            if tok is None: continue\n",
    "            dynamic_window = rnd.randint(1, win) if dyn else win\n",
    "            start, end = max(0, i-dynamic_window), min(i+dynamic_window+1, len_tokens)\n",
    "            for n in xrange(start, end):\n",
    "                if n == i: continue\n",
    "                neighbor_word = tokens[n]+\"_\"+str(n-i) if pos else tokens[n]\n",
    "                pairs.append((tok, neighbor_word))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokens = [\"mary\", \"had\", \"a\", \"little\", \"lamb\"]\n",
    "token_len = len(tokens)\n",
    "for i, tok in enumerate(tokens):\n",
    "    start, end = max(0, i-2), min(i+2+1, token_len)\n",
    "    output = '\\n'.join([row for row in [tok + ' ' + tokens[j] + '_' + str(j - i) for j in xrange(start, end) if j != i and tokens[j] is not None] if len(row) > 0]).strip()\n",
    "    #output = '\\n'.join([row for row in [tok + ' ' + tokens[j] for j in xrange(start, end) if j != i and tokens[j] is not None] if len(row) > 0]).strip()\n",
    "    print \"====================\"\n",
    "    print tok\n",
    "    print output\n",
    "    print \"====================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pairs2counts.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.05 s, sys: 1.17 s, total: 10.2 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%time paircounts = Counter(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9190262\n"
     ]
    }
   ],
   "source": [
    "print len(paircounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counts2vocab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcounts = Counter()\n",
    "contextcounts = Counter()\n",
    "for (word, context), count in paircounts.items():\n",
    "    wordcounts[word] += count\n",
    "    contextcounts[context] += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = sorted(wordcounts.items(), key = lambda (k, v): v, reverse = True)\n",
    "contexts = sorted(contextcounts.items(), key = lambda (k, v): v, reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11815, 11815, 23630)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(contexts), len(words)+len(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counts2pmi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, dok_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiply_by_rows(matrix, row_coefs):\n",
    "    normalizer = dok_matrix((len(row_coefs), len(row_coefs)))\n",
    "    normalizer.setdiag(row_coefs)\n",
    "    return normalizer.tocsr().dot(matrix)\n",
    "\n",
    "def multiply_by_columns(matrix, col_coefs):\n",
    "    normalizer = dok_matrix((len(col_coefs), len(col_coefs)))\n",
    "    normalizer.setdiag(col_coefs)\n",
    "    return matrix.dot(normalizer.tocsr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iw = sorted(map(lambda (w, count): w, words))\n",
    "ic = sorted(map(lambda (c, count): c, contexts))\n",
    "wi = dict([(w, i) for i, w in enumerate(iw)])\n",
    "ci = dict([(c, i) for i, c in enumerate(ic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 175 ms, total: 10.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "counts = dok_matrix((len(wi), len(ci)), dtype = np.float32)\n",
    "for (word, context), count in paircounts.items():\n",
    "    if word in wi and context in ci:\n",
    "        counts[wi[word], ci[context]] = count\n",
    "%time counts = counts.tocsr()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## understand code\n",
    "sum_w = np.array(counts.sum(axis=1))[:, 0] \n",
    "sum_c = np.array(counts.sum(axis=0))[0, :]\n",
    "\n",
    "print sum_w[:10]\n",
    "print map(wordcounts.get, [iw[i] for i in xrange(10)])\n",
    "\n",
    "print sum_c[:10]\n",
    "print map(contextcounts.get, [ic[i] for i in xrange(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 173 ms, sys: 35.9 ms, total: 208 ms\n",
      "Wall time: 208 ms\n",
      "CPU times: user 220 ms, sys: 22.2 ms, total: 242 ms\n",
      "Wall time: 242 ms\n",
      "CPU times: user 15.6 ms, sys: 21.1 ms, total: 36.7 ms\n",
      "Wall time: 36.9 ms\n"
     ]
    }
   ],
   "source": [
    "## calculate e^PMI; PMI without the log()\n",
    "sum_w = np.array(counts.sum(axis=1))[:, 0] \n",
    "sum_c = np.array(counts.sum(axis=0))[0, :]\n",
    "assert np.sum(sum_w) == np.sum(sum_c)\n",
    "if cds != 1.:\n",
    "    sum_c = sum_c ** cds\n",
    "sum_total = sum_c.sum() \n",
    "sum_w = np.reciprocal(sum_w)\n",
    "sum_c = np.reciprocal(sum_c)\n",
    "## the total is not the total, smoothed version of total - D\n",
    "## so the #(w) is not smoothed, #(c) and D are smoothed\n",
    "\n",
    "pmi = csr_matrix(counts)\n",
    "%time pmi = multiply_by_rows(pmi, sum_w)\n",
    "%time pmi = multiply_by_columns(pmi, sum_c)\n",
    "%time pmi = pmi * sum_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pmi2svd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sparsesvd import sparsesvd\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_rows(m):\n",
    "    m2 = m.copy()\n",
    "    m2.data **= 2\n",
    "    norm = np.reciprocal(np.sqrt(np.array(m2.sum(axis=1))[:, 0]))\n",
    "    normalizer = dok_matrix((len(norm), len(norm)))\n",
    "    normalizer.setdiag(norm)\n",
    "    return normalizer.tocsr().dot(self.m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explicit_pmi = pmi.copy()\n",
    "explicit_pmi.data = np.log(pmi.data) - np.log(neg)\n",
    "explicit_pmi.data[explicit_pmi.data < 0] = 0\n",
    "explicit_pmi.eliminate_zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 440 ms, total: 1min 18s\n",
      "Wall time: 1min 18s\n",
      "CPU times: user 3.17 ms, sys: 600 µs, total: 3.77 ms\n",
      "Wall time: 3.77 ms\n"
     ]
    }
   ],
   "source": [
    "## CHOICE 1 - SVD by sparsesvd\n",
    "%time ut, s, vt = sparsesvd(explicit_pmi.tocsc(), dim, )\n",
    "## vector representations are rows of wordvecs\n",
    "%time wordvecs = np.power(s, eig) * ut.T ## scaling the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 21s, sys: 9min 50s, total: 18min 12s\n",
      "Wall time: 35.8 s\n",
      "CPU times: user 8.26 ms, sys: 0 ns, total: 8.26 ms\n",
      "Wall time: 8.22 ms\n"
     ]
    }
   ],
   "source": [
    "## CHOICE 2 - SVD by sklearn - arpack\n",
    "#from scipy.sparse import linalg\n",
    "from sklearn.utils import arpack\n",
    "from sklearn.utils import extmath\n",
    "\n",
    "%time u, s, vt = arpack.svds(explicit_pmi.tocsr(), k = dim)\n",
    "s = s[::-1]\n",
    "u, vt = extmath.svd_flip(u[:, ::-1], vt[::-1])\n",
    "\n",
    "%time wordvecs = np.power(s, eig) * u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 22.9 s, total: 1min 54s\n",
      "Wall time: 1min 1s\n",
      "CPU times: user 4.56 ms, sys: 0 ns, total: 4.56 ms\n",
      "Wall time: 4.47 ms\n"
     ]
    }
   ],
   "source": [
    "## CHOICE 3 - SVD by sklearn - randomized svd\n",
    "from sklearn.utils import extmath\n",
    "%time u, s, vt = extmath.randomized_svd(explicit_pmi.tocsr(), dim, n_iter=10)\n",
    "%time wordvecs = np.power(s, eig) * u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 s, sys: 2.06 s, total: 18.9 s\n",
      "Wall time: 18.6 s\n",
      "CPU times: user 1.51 s, sys: 46.8 ms, total: 1.56 s\n",
      "Wall time: 1.56 s\n",
      "CPU times: user 653 µs, sys: 743 µs, total: 1.4 ms\n",
      "Wall time: 1.4 ms\n",
      "(11815, 200) (200,)\n",
      "CPU times: user 2.71 ms, sys: 3.85 ms, total: 6.56 ms\n",
      "Wall time: 6.61 ms\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "## CHOICE 4 - SVD by spark\n",
    "nrows, ncols = explicit_pmi.shape\n",
    "rows, cols = explicit_pmi.nonzero()\n",
    "with open(\"data/tmp/pmi.txt\", \"w\") as f:\n",
    "    f.write(\"# %d %d\\n\" % (nrows, ncols)) # dimension\n",
    "    %time f.write(\"\\n\".join([\"%d,%d,%g\" % (r, c, d) for r, c, d in zip(rows, cols, explicit_pmi.data)]))\n",
    "    \n",
    "    \n",
    "### run spark code - looking at data/tmp/U.txt data/tmp/s.txt\n",
    "!/usr/spark/bin/spark-shell --master local[20] --driver-memory 50g -i rowmatrixSVD.scala\n",
    "\n",
    "## load back matrices\n",
    "%time u = np.loadtxt(\"data/tmp/U.txt\", delimiter=\",\")\n",
    "%time s = np.loadtxt(\"data/tmp/s.txt\", delimiter=\",\")\n",
    "print u.shape, s.shape\n",
    "%time wordvecs = np.power(s, eig) * u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11815, 200)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## normalize word vectors\n",
    "from sklearn import preprocessing\n",
    "normalized_wordvecs = preprocessing.normalize(wordvecs, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.55885984,  0.21545189,  0.11841743, ...,  0.0013745 ,\n",
       "        -0.02271895,  0.04978845],\n",
       "       [-0.21680112,  0.01875686,  0.05588003, ..., -0.0771681 ,\n",
       "        -0.03484708, -0.02163088],\n",
       "       [-0.19255549, -0.19141807,  0.16705572, ...,  0.06263481,\n",
       "         0.06455367, -0.05304903],\n",
       "       ..., \n",
       "       [-0.17444888, -0.10237271,  0.01706275, ...,  0.02818981,\n",
       "        -0.04560365, -0.00398814],\n",
       "       [-0.20024668, -0.10312052, -0.10649699, ..., -0.03765361,\n",
       "         0.04984179,  0.0461817 ],\n",
       "       [-0.16624444, -0.09761244, -0.02911183, ..., -0.02920824,\n",
       "         0.09424746, -0.04249255]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layout ['layout', 'rows', 'keys', 'diatonic', 'chromatic', 'tile', 'row', 'keyboard', 'harmonica', 'placement']\n",
      "\n",
      "taiwan ['taiwan', 'korea', 'china', 'prc', 'singapore', 'malaysia', 'shanghai', 'hong', 'chiang', 'thailand']\n",
      "\n",
      "operative ['operative', 'lodge', 'freemasonry', 'organisation', 'membership', 'recognition', 'benefits', 'societies', 'participation', 'masonic']\n",
      "\n",
      "studios ['studios', 'theaters', 'bros', 'productions', 'studio', 'movies', 'warner', 'films', 'movie', 'cinema']\n",
      "\n",
      "mexican ['mexican', 'jos', 'luis', 'rican', 'argentine', 'juan', 'manuel', 'cruz', 'santiago', 'pedro']\n",
      "\n",
      "organ ['organ', 'harpsichord', 'cello', 'violin', 'sonata', 'concerto', 'organs', 'orchestral', 'piano', 'quartet']\n",
      "\n",
      "narrator ['narrator', 'cameo', 'novel', 'dracula', 'kubrick', 'story', 'comedy', 'sequel', 'vampire', 'tale']\n",
      "\n",
      "usefulness ['usefulness', 'tool', 'applications', 'users', 'difficult', 'fix', 'need', 'easy', 'reasons', 'discussion']\n",
      "\n",
      "diagnosis ['diagnosis', 'diagnostic', 'symptoms', 'disorders', 'clinical', 'disorder', 'patients', 'patient', 'syndrome', 'diagnosed']\n",
      "\n",
      "suspension ['suspension', 'wheels', 'truck', 'motorcycle', 'chassis', 'bike', 'bicycle', 'wheel', 'motorcycles', 'cars']\n",
      "\n",
      "humanist ['humanist', 'philosopher', 'theologian', 'reformer', 'naturalist', 'giovanni', 'erasmus', 'francesco', 'physician', 'humanism']\n",
      "\n",
      "stretch ['stretch', 'stretches', 'canals', 'navigable', 'river', 'canal', 'longest', 'inland', 'stretching', 'connects']\n",
      "\n",
      "int ['int', 'lim', 'infty', 'qquad', 'cos', 'cdot', 'mathrm', 'frac', 'quad', 'tau']\n",
      "\n",
      "allowing ['allowing', 'automatically', 'allows', 'allow', 'move', 'quickly', 'simultaneously', 'advantage', 'switch', 'programs']\n",
      "\n",
      "territory ['territory', 'territories', 'territorial', 'annexed', 'sovereignty', 'ceded', 'uninhabited', 'islands', 'lands', 'province']\n",
      "\n",
      "gaza ['gaza', 'palestinian', 'plo', 'palestinians', 'israeli', 'lebanon', 'golan', 'israel', 'hamas', 'israelis']\n",
      "\n",
      "cover ['cover', 'album', 'lp', 'charts', 'titled', 'single', 'discography', 'covered', 'soundtrack', 'wonder']\n",
      "\n",
      "kate ['kate', 'actress', 'songwriter', 'amy', 'musician', 'marilyn', 'singer', 'filmmaker', 'jennifer', 'kelly']\n",
      "\n",
      "ridge ['ridge', 'hills', 'mount', 'rocky', 'glacier', 'mountain', 'slopes', 'canyon', 'rugged', 'glaciers']\n",
      "\n",
      "roberts ['roberts', 'craig', 'chris', 'walker', 'ian', 'smith', 'ron', 'glenn', 'kevin', 'tony']\n",
      "\n",
      "dissertation ['dissertation', 'doctoral', 'thesis', 'ph', 'doctorate', 'princeton', 'harvard', 'mit', 'undergraduate', 'graduate']\n",
      "\n",
      "shepherd ['shepherd', 'hound', 'dog', 'laurence', 'gordon', 'goat', 'nancy', 'arnold', 'farmer', 'bennett']\n",
      "\n",
      "prone ['prone', 'hazards', 'floods', 'severe', 'earthquakes', 'disasters', 'cause', 'flooding', 'storms', 'degradation']\n",
      "\n",
      "dye ['dye', 'leds', 'ink', 'solid', 'substrate', 'ultraviolet', 'colored', 'transparent', 'inorganic', 'light']\n",
      "\n",
      "hypoglycemia ['hypoglycemia', 'diabetes', 'symptoms', 'insulin', 'seizures', 'glucose', 'patients', 'deficiency', 'acute', 'hormone']\n",
      "\n",
      "offense ['offense', 'scrimmage', 'yards', 'touchdown', 'penalties', 'penalty', 'offence', 'kick', 'quarterback', 'yard']\n",
      "\n",
      "lived ['lived', 'moved', 'parents', 'emigrated', 'father', 'settled', 'died', 'who', 'life', 'mother']\n",
      "\n",
      "stake ['stake', 'heretic', 'subsidiary', 'hus', 'heretics', 'company', 'holdings', 'heresy', 'chrysler', 'condemned']\n",
      "\n",
      "attention ['attention', 'critical', 'readers', 'subject', 'influence', 'subjective', 'psychology', 'cognitive', 'focus', 'interested']\n",
      "\n",
      "tunes ['tunes', 'jazz', 'music', 'folk', 'melodic', 'rhythmic', 'songs', 'funk', 'blues', 'musicians']\n",
      "\n",
      "administrator ['administrator', 'chief', 'governor', 'administrators', 'deputy', 'appointed', 'cabinet', 'dns', 'client', 'minister']\n",
      "\n",
      "expenses ['expenses', 'paid', 'tax', 'pay', 'payments', 'debts', 'funds', 'budget', 'paying', 'income']\n",
      "\n",
      "whereby ['whereby', 'vested', 'exercised', 'executive', 'representative', 'parliamentary', 'framework', 'judiciary', 'constitutional', 'legislative']\n",
      "\n",
      "journalism ['journalism', 'academic', 'media', 'undergraduate', 'writing', 'magazines', 'curriculum', 'newspapers', 'graduate', 'journals']\n",
      "\n",
      "touchdown ['touchdown', 'quarterback', 'yards', 'bengals', 'browns', 'scrimmage', 'yard', 'nfl', 'ravens', 'offense']\n",
      "\n",
      "asked ['asked', 'replied', 'refused', 'asking', 'saying', 'did', 'ask', 'him', 'said', 'answered']\n",
      "\n",
      "corinth ['corinth', 'aegean', 'crete', 'athens', 'ephesus', 'athenian', 'herodotus', 'greece', 'alexandria', 'plutarch']\n",
      "\n",
      "employees ['employees', 'employee', 'employers', 'corporate', 'business', 'employer', 'shareholders', 'corporations', 'companies', 'employment']\n",
      "\n",
      "slave ['slave', 'slaves', 'slavery', 'plantation', 'blacks', 'africans', 'whites', 'douglass', 'emancipation', 'african']\n",
      "\n",
      "residing ['residing', 'census', 'households', 'population', 'migrant', 'hispanic', 'median', 'ethnicity', 'demographics', 'sq']\n",
      "\n",
      "factories ['factories', 'industrial', 'factory', 'manufacturing', 'manufacturers', 'goods', 'construction', 'enterprises', 'owned', 'mining']\n",
      "\n",
      "professors ['professors', 'graduate', 'undergraduate', 'faculty', 'students', 'doctorate', 'harvard', 'academics', 'universities', 'cornell']\n",
      "\n",
      "voting ['voting', 'voter', 'votes', 'candidates', 'ballot', 'vote', 'voters', 'plurality', 'candidate', 'electoral']\n",
      "\n",
      "promotional ['promotional', 'artwork', 'advertisements', 'videos', 'selling', 'albums', 'manson', 'cds', 'album', 'studio']\n",
      "\n",
      "vulnerable ['vulnerable', 'effective', 'combat', 'attack', 'targets', 'tactical', 'tanks', 'ineffective', 'deadly', 'equipped']\n",
      "\n",
      "terms ['terms', 'term', 'definition', 'definitions', 'define', 'is', 'concepts', 'defined', 'context', 'usage']\n",
      "\n",
      "admiral ['admiral', 'hms', 'naval', 'fleet', 'beatty', 'navy', 'commander', 'admiralty', 'marshal', 'sunk']\n",
      "\n",
      "many ['many', 'some', 'most', 'other', 'have', 'these', 'such', 'more', 'often', 'also']\n",
      "\n",
      "records ['records', 'record', 'albums', 'album', 'lp', 'remix', 'discography', 'label', 'recorded', 'dre']\n",
      "\n",
      "greenland ['greenland', 'faroe', 'iceland', 'denmark', 'arctic', 'scandinavia', 'danish', 'inuit', 'nordic', 'atlantic']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = np.random.randint(0, len(iw), 50)\n",
    "for w in np.array(iw)[s]:\n",
    "#for w in [\"east\", \"china\", \"money\", \"football\", \"library\"]:\n",
    "#for w in [\"tiger\", \"cat\", \"dog\", \"bear\", \"hamster\", \"mammal\", \"gem\", \"drink\", \"king\", \"queen\"]:\n",
    "    v = normalized_wordvecs[wi[w]]\n",
    "    scores = normalized_wordvecs.dot(v)\n",
    "    closest = sorted(enumerate(scores), key = lambda (i,s):s, reverse=True)[:10]\n",
    "    print w, [iw[i] for (i, _) in closest]\n",
    "    print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['man', 'queen', 'girl', 'love', 'my', 'woman', 'her', 'friends', 'she', 'thinks']\n"
     ]
    }
   ],
   "source": [
    "def vec(w): return normalized_wordvecs[wi[w], :]\n",
    "analogy = [\"king\", \"man\", \"queen\", \"woman\"] #[\"france\", \"paris\", \"uk\", \"london\"]\n",
    "a, b, aa, bb = map(vec, analogy)\n",
    "bb_estimate = b - a + aa\n",
    "bb_estimate /= np.sqrt(np.sum(bb_estimate**2))\n",
    "scores = normalized_wordvecs.dot(bb_estimate)\n",
    "closest = sorted(enumerate(scores), key = lambda (i,s):s, reverse=True)[:10]\n",
    "print [iw[i] for (i, _) in closest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
