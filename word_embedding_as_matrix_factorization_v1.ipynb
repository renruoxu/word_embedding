{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Word Vectors\n",
    "- [Neural Word Embedding as Implicit Matrix Factorization](http://u.cs.biu.ac.il/~nlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf)\n",
    "- [text8 data](http://mattmahoney.net/dc/text8.zip)\n",
    "- [wordsim353](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/)\n",
    "- [MEN Test collection](http://clic.cimec.unitn.it/~elia.bruni/MEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.45 s, sys: 452 ms, total: 3.91 s\n",
      "Wall time: 3.91 s\n",
      "17005207\n"
     ]
    }
   ],
   "source": [
    "corpus = open(\"data/text8\").read()\n",
    "pat = re.compile(\"\\w+\")\n",
    "%time words = pat.findall(corpus)\n",
    "print len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.72 s, sys: 320 ms, total: 5.04 s\n",
      "Wall time: 4.93 s\n",
      "0.0465425008076 % data are retained\n",
      "15471435\n"
     ]
    }
   ],
   "source": [
    "%time total_word_counter = Counter(words)\n",
    "WC_THR = 100\n",
    "freq_words = set([w for w,c in total_word_counter.items() if c >= WC_THR])\n",
    "print len(freq_words) *1. / len(total_word_counter), \"% data are retained\"\n",
    "words = [w for w in words if w in freq_words]\n",
    "print len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hasher(object):\n",
    "    def __init__(self):\n",
    "        self.N = 0\n",
    "        self.data = {}\n",
    "    def hash(self,item):\n",
    "        if item not in self.data:\n",
    "            self.data[item] = self.N\n",
    "            self.N += 1\n",
    "        return self.data[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_wc_pairs(words, window = 3):\n",
    "    wcpairs = []\n",
    "    wordhasher = Hasher()\n",
    "    contexthasher = Hasher()\n",
    "    for i in xrange(window, len(words)-window):\n",
    "        word, context = words[i], tuple(words[i-window:i]+words[i+1:i+window+1])\n",
    "        hw, hc = wordhasher.hash(word), contexthasher.hash(context)\n",
    "        wcpairs.append( (hw, hc) )\n",
    "    return wordhasher, contexthasher, wcpairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.1 s, sys: 2.26 s, total: 29.4 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%time wordhasher, contexthasher, wcpairs = extract_wc_pairs(words, window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.9 s, sys: 1.93 s, total: 36.8 s\n",
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "%time whs, chs = zip(*wcpairs)\n",
    "whs, chs = np.asarray(whs), np.asarray(chs)\n",
    "Mshape = (wordhasher.N, contexthasher.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.92 s, sys: 99.1 ms, total: 5.01 s\n",
      "Wall time: 4.88 s\n",
      "CPU times: user 4.56 s, sys: 410 ms, total: 4.97 s\n",
      "Wall time: 4.97 s\n",
      "CPU times: user 26 s, sys: 805 ms, total: 26.8 s\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%time wcounter = Counter(whs)\n",
    "%time ccounter = Counter(chs)\n",
    "D = len(wcpairs)\n",
    "k = 10.\n",
    "%time data = np.array([D*1./wcounter[wh]/ccounter[ch]/k for wh,ch in wcpairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 128 ms, sys: 85.6 ms, total: 214 ms\n",
      "Wall time: 213 ms\n",
      "CPU times: user 1.24 s, sys: 43.5 ms, total: 1.29 s\n",
      "Wall time: 1.29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11815, 13132213)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time M = sparse.coo_matrix((data, (whs, chs)), shape = Mshape, dtype=np.float32)\n",
    "%time M = M.tocsr()\n",
    "M.data = np.log(M.data)##\n",
    "M[M<0.0] = 0.0\n",
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 56s, sys: 35 s, total: 6min 31s\n",
      "Wall time: 6min 32s\n"
     ]
    }
   ],
   "source": [
    "%time wvectors = svd.fit_transform(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check word-context matrix & word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4771\n",
      "(1211, 4731)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('creation', ('then', 'begin', 'one', 's'))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print data.argmax()\n",
    "print wcpairs[4771]\n",
    "h2w[1211], h2c[4731]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAEACAYAAACXqUyYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcc381200d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.boxplot(M.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-447.070925306 883.888893924\n",
      "[-0.013965013009280488, -0.00038266280651075247, 0.00010446357300787138, 0.0028947099433689655, 0.037908273507591878]\n"
     ]
    }
   ],
   "source": [
    "print wvectors.min(), wvectors.max()\n",
    "print np.percentile(wvectors.ravel(), [5, 25, 50, 75, 95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.2 ms, sys: 73.8 ms, total: 138 ms\n",
      "Wall time: 137 ms\n",
      "CPU times: user 38.9 s, sys: 3.81 s, total: 42.7 s\n",
      "Wall time: 42.7 s\n"
     ]
    }
   ],
   "source": [
    "## inverse index\n",
    "%time h2w = dict([(v,k) for k,v in wordhasher.data.items()])\n",
    "%time h2c = dict([(v,k) for k,v in contexthasher.data.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 623 Âµs, sys: 6.81 ms, total: 7.44 ms\n",
      "Wall time: 7.43 ms\n",
      "CPU times: user 16.5 ms, sys: 2.97 ms, total: 19.4 ms\n",
      "Wall time: 19.3 ms\n",
      "CPU times: user 11.5 ms, sys: 1.49 ms, total: 13 ms\n",
      "Wall time: 13.1 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import h5py, cPickle\n",
    "\n",
    "h5f = h5py.File(\"data/word_vectors.h5\", \"w\")\n",
    "%time h5f.create_dataset(\"data\", data = wvectors)\n",
    "h5f.close()\n",
    "\n",
    "%time cPickle.dump(h2w, open(\"data/inverse_word_hash.pkl\", \"w\"))\n",
    "%time cPickle.dump(wordhasher.data, open(\"data/word_hash.pkl\", \"w\"))\n",
    "#%time cPickle.dump(h2c, open(\"data/inverse_context_hash.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py, cPickle\n",
    "h5f = h5py.File(\"data/word_vectors.h5\", \"r\")\n",
    "wvectors = h5f[\"data\"][:]\n",
    "h5f.close()\n",
    "\n",
    "h2w = cPickle.load(open(\"data/inverse_word_hash.pkl\"))\n",
    "w2h = cPickle.load(open(\"data/word_hash.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## use flann to find knn\n",
    "import pyflann as pf\n",
    "from scipy import stats\n",
    "\n",
    "class NearestNeighbor(object):\n",
    "    def __init__(self, k = 5, algorithm=\"kdtree\", distance_type=\"euclidean\"):\n",
    "        pf.set_distance_type(distance_type)\n",
    "        self.flann = pf.FLANN()\n",
    "        self.k = k\n",
    "        self.algorithm = algorithm#\"autotuned\"#algorithm\n",
    "        self.iterations = 1000\n",
    "    def train(self, X):\n",
    "        self.X_ = X\n",
    "    def nearest(self, X):\n",
    "        min_index, dists = self.flann.nn(self.X_, X, self.k, \n",
    "                                         algorithm = self.algorithm, \n",
    "                                         iterations=self.iterations)\n",
    "        return min_index, dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 483 ms, sys: 362 ms, total: 845 ms\n",
      "Wall time: 62.9 ms\n"
     ]
    }
   ],
   "source": [
    "nn=NearestNeighbor()\n",
    "nn.train(wvectors)\n",
    "%time neighbors, dists =nn.nearest(wvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['concerns', 'suggestions', 'besieged', 'dressed', 'surplus']\n",
      "['variant', 'synthesis', 'mix', 'copy', 'wake']\n",
      "['green', 'mysteries', 'calendar', 'basically', 'ethnic']\n",
      "['authors', 'actors', 'refugees', 'peoples', 'europeans']\n",
      "['mary', 'bits', 'vast', 'shape', 'abstract']\n",
      "['proto', 'freud', 'inference', 'introductory', 'streams']\n",
      "['precursor', 'dress', 'divergence', 'regulatory', 'eighteenth']\n",
      "['opposing', 'basques', 'coastal', 'bend', 'malay']\n",
      "['philosophies', 'edu', 'initiation', 'cliffs', 'orbitals']\n",
      "['explains', 'sweeping', 'exchanges', 'qualification', 'intake']\n",
      "['miss', 'epsilon', 'strain', 'restrict', 'zones']\n",
      "['she', 'university', 'south', 'theory', 'around']\n",
      "['wishes', 'rockets', 'superiority', 'heroic', 'extremes']\n",
      "['make', 'henry', 'held', 'meters', 'eventually']\n",
      "['privilege', 'veterans', 'im', 'employers', 'prefixes']\n",
      "['whereby', 'aiming', 'observing', 'auxiliary', 'honors']\n",
      "['proper', 'aftermath', 'rational', 'naval', 'expulsion']\n",
      "['believes', 'asserts', 'leone', 'nacional', 'tasmania']\n",
      "['entirely', 'precise', 'primitive', 'pejorative', 'historian']\n",
      "['hold', 'suburb', 'contact', 'descendant', 'preface']\n",
      "['always', 'started', 'lake', 'winter', 'philippines']\n",
      "['exist', 'seem', 'foreigners', 'speeches', 'israelis']\n",
      "['highly', 'sum', 'prominent', 'remarkable', 'multitude']\n",
      "['women', 'particularly', 'brought', 'basic', 'complete']\n",
      "['day', 'less', 'major', 'president', 'local']\n",
      "['growing', 'substantial', 'vast', 'huge', 'crucial']\n",
      "['quiet', 'antarctica', 'bisexual', 'conjugation', 'suspicious']\n",
      "['rumors', 'banach', 'lotus', 'dave', 'sim']\n",
      "['reader', 'crusaders', 'clan', 'moors', 'cat']\n",
      "['spread', 'distribution', 'copy', 'domain', 'nucleus']\n",
      "['various', 'rights', 'island', 'region', 'died']\n",
      "['kinds', 'deficiency', 'exciting', 'arid', 'ber']\n",
      "['broader', 'aesthetics', 'chips', 'spaces', 'sexuality']\n",
      "['popularized', 'programmed', 'referenced', 'addressed', 'deployed']\n",
      "['books', 'particular', 'trade', 'typically', 'republic']\n",
      "['articles', 'aim', 'branches', 'weapons', 'legislative']\n",
      "['website', 'grammar', 'phenomenon', 'lesser', 'passing']\n",
      "['murray', 'robots', 'vegetables', 'enthusiasm', 'rolled']\n",
      "['predominantly', 'radicals', 'arising', 'mistress', 'stationed']\n",
      "['tradition', 'circle', 'vote', 'centre', 'provinces']\n",
      "['desires', 'injection', 'connector', 'consortium', 'accommodate']\n",
      "['branches', 'opponents', 'contemporaries', 'cult', 'chemistry']\n",
      "['consequently', 'immortal', 'kinds', 'torpedo', 'monarchies']\n",
      "['recognise', 'lutherans', 'emigrated', 'edwards', 'ya']\n",
      "['synthesis', 'mix', 'parody', 'outline', 'residence']\n",
      "['classical', 'washington', 'results', 'settlement', 'lines']\n",
      "['liberalism', 'honda', 'cobol', 'thermodynamics', 'angola']\n",
      "['austrian', 'swedish', 'norwegian', 'mexican', 'israeli']\n",
      "['capitalist', 'mysterious', 'rabbinic', 'ubiquitous', 'monastic']\n",
      "['defines', 'goldwater', 'phones', 'elisha', 'phil']\n",
      "['terms', 'lower', 'season', 'leader', 'middle']\n",
      "['aggression', 'craters', 'herbs', 'tricks', 'temporal']\n",
      "['concept', 'creation', 'discovery', 'idea', 'product']\n",
      "['david', 'george', 'independence', 'henry', 'remembered']\n",
      "['friedman', 'rumsfeld', 'commissioners', 'panda', 'cromwell']\n",
      "['jan', 'sep', 'oct', 'nov', 'mar']\n",
      "['rand', 'amy', 'pike', 'lancaster', 'scratch']\n",
      "['capitalists', 'vinci', 'pngimage', 'dinger', 'ranged']\n",
      "['historians', 'innovations', 'hypotheses', 'breach', 'casualties']\n",
      "['hart', 'tanker', 'senegal', 'blindness', 'anatolia']\n",
      "['ralph', 'mel', 'laurence', 'morrison', 'andreas']\n",
      "['gustave', 'wooden', 'twisted', 'nacional', 'maintaining']\n",
      "['opponents', 'branches', 'criticisms', 'authorship', 'princes']\n",
      "['dispute', 'canal', 'bombing', 'revolt', 'strip']\n",
      "['these', 'must', 'created', 'culture', 'europe']\n",
      "['whether', 'baseball', 'marked', 'ended', 'focused']\n",
      "['controversial', 'expensive', 'concentrated', 'essential', 'critical']\n",
      "['debate', 'neighborhood', 'eye', 'iliad', 'vowels']\n",
      "['countries', 'israel', 'star', 'close', 'taking']\n",
      "['taking', 'ada', 'python', 'pop', 'backed']\n",
      "['deep', 'yellow', 'flaws', 'historian', 'instructions']\n",
      "['ecology', 'lasers', 'cantonese', 'yiddish', 'burton']\n",
      "['biodiversity', 'humidity', 'slots', 'guitarists', 'celebrity']\n",
      "['earth', 'head', 'side', 'study', 'nearly']\n",
      "['destroying', 'mvp', 'invading', 'ensured', 'oceania']\n",
      "['particular', 'whether', 'women', 'basic', 'makes']\n",
      "['importance', 'establishment', 'application', 'decline', 'discovery']\n",
      "['takes', 'nights', 'verse', 'chapters', 'phases']\n",
      "['tree', 'instrument', 'calendar', 'wall', 'genre']\n",
      "['sitting', 'exhibited', 'voluntary', 'somewhere', 'discarded']\n",
      "['component', 'interpretation', 'implementation', 'sign', 'explanation']\n",
      "['sees', 'isolation', 'seizures', 'registry', 'finding']\n",
      "['domination', 'recipients', 'dynamics', 'institutions', 'tragedy']\n",
      "['metaphor', 'viewpoint', 'radius', 'regulation', 'tag']\n",
      "['involves', 'defeating', 'butler', 'diagnostic', 'marble']\n",
      "['critique', 'complications', 'habitat', 'reporter', 'displacement']\n",
      "['itself', 'reference', 'll', 'hz', 'animated']\n",
      "['return', 'lines', 'color', 'schools', 'administration']\n",
      "['pre', 'mainstream', 'plural', 'precise', 'keeping']\n",
      "['usually', 'germany', 'federal', 'top', 'austrian']\n",
      "['agricultural', 'investment', 'strict', 'engineers', 'counts']\n",
      "['develops', 'joins', 'challenging', 'regain', 'converts']\n",
      "['technology', 'practice', 'lake', 'engine', 'say']\n",
      "['themes', 'structures', 'reforms', 'hook', 'entity']\n",
      "['present', 'site', 'civil', 'henry', 'key']\n",
      "['writings', 'soul', 'relationship', 'entry', 'biography']\n",
      "['jean', 'latvia', 'boeing', 'sp', 'steve']\n",
      "['jacques', 'capoeira', 'walking', 'summers', 'luthor']\n",
      "['rousseau', 'bismarck', 'kubrick', 'marvin', 'jackie']\n",
      "['context', 'face', 'translation', 'origins', 'composition']\n"
     ]
    }
   ],
   "source": [
    "for row in neighbors[1000:1100, :]:\n",
    "    print [h2w[r] for r in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
